[core]
# Set timezone
default_timezone = Asia/Jakarta
executor = LocalExecutor

# Dynamic paths (will be set via environment variables)
dags_folder = /mnt/c/Users/MacBook/OneDrive/Dokumen/VS_Code/magnitudr/airflow/dags
plugins_folder = /mnt/c/Users/MacBook/OneDrive/Dokumen/VS_Code/magnitudr/airflow/plugins

# Performance optimizations
load_examples = False
execute_tasks_new_python_interpreter = False
parallelism = 32
max_active_tasks_per_dag = 16
max_active_runs_per_dag = 1
dagbag_import_timeout = 30.0

# DAG parsing optimization
dag_dir_list_interval = 300
dag_file_parsing_timeout = 30
min_file_process_interval = 30
store_serialized_dags = True
min_serialized_dag_update_interval = 30
compress_serialized_dags = True

[webserver]
# Web server settings
web_server_port = 8080
base_url = http://localhost:8080
secret_key = earthquake-pipeline-secret-key-2025

# Security settings - Simplified for development
expose_config = True
authenticate = False
auth_backend = airflow.api.auth.backend.default

# Performance settings
workers = 4
worker_refresh_batch_size = 1
worker_refresh_interval = 6000
reload_on_plugin_change = False

[scheduler]
# Scheduler optimization for better performance
dag_dir_list_interval = 60
dag_file_parsing_timeout = 30
min_file_process_interval = 10
catchup_by_default = False

# Performance tuning
max_tis_per_query = 512
parsing_processes = 2
scheduler_heartbeat_sec = 5
job_heartbeat_sec = 5
num_runs = -1

# Task instance optimization
max_dagruns_to_create_per_loop = 10
max_dagruns_per_loop_to_schedule = 20

[database]
# Use PostgreSQL instead of SQLite (production ready)
sql_alchemy_conn = postgresql://airflow_user:airflow_pass@localhost:5432/airflow_db
sql_alchemy_pool_size = 10
sql_alchemy_pool_recycle = 3600
sql_alchemy_pool_pre_ping = True

[logging]
# Logging configuration
base_log_folder = /mnt/c/Users/MacBook/OneDrive/Dokumen/VS_Code/magnitudr/airflow/logs
remote_logging = False
logging_level = INFO
fab_logging_level = WARN
logging_config_class = 
colored_console_log = True
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

[api]
# Enable API
auth_backends = airflow.api.auth.backend.default
maximum_page_limit = 100

[celery]
# Celery configuration (if needed)
worker_concurrency = 4
task_always_eager = False

[sensors]
# Sensor optimization
default_timeout = 604800
sensor_default_timeout = 604800
default_retry_delay = 60
default_email_on_retry = False
default_email_on_failure = False

[taskflow]
# TaskFlow API settings
decorated_operator_class = airflow.operators.python.PythonOperator

[operators]
# Operator defaults
default_owner = magnitudr-team
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0
